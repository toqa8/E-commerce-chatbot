# -*- coding: utf-8 -*-
"""prompt_template_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c-mkvfx3Mak3t-7H7IlpJsNcfUoWc0NF
"""

!pip install -U langchain langchain-community langchain-huggingface chromadb sentence-transformers

"""**Running the embeddings.py file**"""

import pandas as pd
from langchain_community.vectorstores import Chroma # a vectore store to store embeddings and metadata
from langchain.docstore.document import Document # a standard format for storing text and metadata
from sentence_transformers import SentenceTransformer # used to turn text into embeddings using pre-trained modells like "all-mpnet-base-v2"
from langchain_huggingface import HuggingFaceEmbeddings
import os

# Load the dataset
data = pd.read_csv(r"/content/qna_dataset.csv")

# Creates a new column "Combined" Combining Question and Answer into a single text
data["Combined"] = data.apply(
    lambda row: f"question: {row['question']} answer: {row['answer']}", axis=1
)

# Create documents for Chroma, instead of plain text from the combined text, for more flexibility for more use cases and for working smoothly in langchain ecosystem
documents = [
    Document(page_content=row["Combined"], metadata={"question": row["question"], "answer": row["answer"]})
    for _, row in data.iterrows()
]

# Load the embedding model using LangChain's wrapper
embedding_function = HuggingFaceEmbeddings(model_name="all-mpnet-base-v2")

try:
# Create the vector database using from_documents
    vector_db = Chroma.from_documents(
        documents,
        embedding=embedding_function,  # Correct: pass the wrapper, not just a function
        persist_directory="db"
    )
    print('Vector store created successfully')
except Exception as e:
    print(f"error: {e}")

print("Saving DB to:", os.path.abspath("db"))

"""**Test**"""

from langchain.prompts import PromptTemplate
import pandas as pd

# Import your prompt template function
def get_prompt_template():
    template = """
You are a helpful e-commerce assistant. Use the following question-answer pairs (context) to help answer the user's question.

{context}

User's Question: {query}

Provide a clear and helpful answer based on the information above.
"""
    return PromptTemplate(input_variables=["context", "query"], template=template)

# Simulate user input
user_query = "How long does delivery take?"

# Use the vector database to retrieve relevant docs
retrieved_docs = vector_db.similarity_search(user_query)

# Build the context from retrieved documents
context = "\n".join([doc.page_content for doc in retrieved_docs])

# Get the prompt template and format it
prompt_template = get_prompt_template()
formatted_prompt = prompt_template.format(context=context, query=user_query)

# Print the formatted prompt
print(formatted_prompt)